{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import pdb\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STYLE_LAYERS = ('conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1')\n",
    "CONTENT_LAYER = 'conv4_2'\n",
    "\n",
    "MEAN_PIXEL = np.array([ 123.68 , 116.779, 103.939])\n",
    "\n",
    "CONTENT_WEIGHT = 7.5e0\n",
    "STYLE_WEIGHT = 1e2\n",
    "TV_WEIGHT = 2e2\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHTS_INIT_STDEV = .1\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _weights(vgg_layers, layer, expected_layer_name):\n",
    "    \"\"\" Return the weights and biases already trained by VGG\n",
    "    \"\"\"\n",
    "    W = vgg_layers[0][layer][0][0][2][0][0]\n",
    "    b = vgg_layers[0][layer][0][0][2][0][1]\n",
    "    layer_name = vgg_layers[0][layer][0][0][0][0]\n",
    "    assert layer_name == expected_layer_name\n",
    "    return W, b.reshape(b.size)\n",
    "\n",
    "def _conv2d_relu(vgg_layers, prev_layer, layer, layer_name):\n",
    "    \"\"\" Return the Conv2D layer with RELU using the weights, biases from the VGG\n",
    "    model at 'layer'.\n",
    "    Inputs:\n",
    "        vgg_layers: holding all the layers of VGGNet\n",
    "        prev_layer: the output tensor from the previous layer\n",
    "        layer: the index to current layer in vgg_layers\n",
    "        layer_name: the string that is the name of the current layer.\n",
    "                    It's used to specify variable_scope.\n",
    "    Output:\n",
    "        relu applied on the convolution.\n",
    "    Note that you first need to obtain W and b from vgg-layers using the function\n",
    "    _weights() defined above.\n",
    "    W and b returned from _weights() are numpy arrays, so you have\n",
    "    to convert them to TF tensors using tf.constant.\n",
    "    Note that you'll have to do apply relu on the convolution.\n",
    "    Hint for choosing strides size: \n",
    "        for small images, you probably don't want to skip any pixel\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(layer_name) as scope:\n",
    "        W, b = _weights(vgg_layers, layer, layer_name)\n",
    "        W = tf.constant(W, name='weights')\n",
    "        b = tf.constant(b, name='bias')\n",
    "        conv2d = tf.nn.conv2d(prev_layer, filter=W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv2d + b)\n",
    "\n",
    "def _avgpool(prev_layer):\n",
    "    \"\"\" Return the average pooling layer. The paper suggests that average pooling\n",
    "    actually works better than max pooling.\n",
    "    Input:\n",
    "        prev_layer: the output tensor from the previous layer\n",
    "    Output:\n",
    "        the output of the tf.nn.avg_pool() function.\n",
    "    Hint for choosing strides and kszie: choose what you feel appropriate\n",
    "    \"\"\"\n",
    "    return tf.nn.avg_pool(prev_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], \n",
    "                          padding='SAME', name='avg_pool_')\n",
    "\n",
    "def load_vgg(path, input_image):\n",
    "    \"\"\" Load VGG into a TensorFlow model.\n",
    "    Use a dictionary to hold the model instead of using a Python class\n",
    "    \"\"\"\n",
    "    vgg = scipy.io.loadmat(path)\n",
    "    vgg_layers = vgg['layers']\n",
    "\n",
    "    graph = {}\n",
    "    graph['conv1_1']  = _conv2d_relu(vgg_layers, input_image, 0, 'conv1_1')\n",
    "    graph['conv1_2']  = _conv2d_relu(vgg_layers, graph['conv1_1'], 2, 'conv1_2')\n",
    "    graph['avgpool1'] = _avgpool(graph['conv1_2'])\n",
    "    graph['conv2_1']  = _conv2d_relu(vgg_layers, graph['avgpool1'], 5, 'conv2_1')\n",
    "    graph['conv2_2']  = _conv2d_relu(vgg_layers, graph['conv2_1'], 7, 'conv2_2')\n",
    "    graph['avgpool2'] = _avgpool(graph['conv2_2'])\n",
    "    graph['conv3_1']  = _conv2d_relu(vgg_layers, graph['avgpool2'], 10, 'conv3_1')\n",
    "    graph['conv3_2']  = _conv2d_relu(vgg_layers, graph['conv3_1'], 12, 'conv3_2')\n",
    "    graph['conv3_3']  = _conv2d_relu(vgg_layers, graph['conv3_2'], 14, 'conv3_3')\n",
    "    graph['conv3_4']  = _conv2d_relu(vgg_layers, graph['conv3_3'], 16, 'conv3_4')\n",
    "    graph['avgpool3'] = _avgpool(graph['conv3_4'])\n",
    "    graph['conv4_1']  = _conv2d_relu(vgg_layers, graph['avgpool3'], 19, 'conv4_1')\n",
    "    graph['conv4_2']  = _conv2d_relu(vgg_layers, graph['conv4_1'], 21, 'conv4_2')\n",
    "    graph['conv4_3']  = _conv2d_relu(vgg_layers, graph['conv4_2'], 23, 'conv4_3')\n",
    "    graph['conv4_4']  = _conv2d_relu(vgg_layers, graph['conv4_3'], 25, 'conv4_4')\n",
    "    graph['avgpool4'] = _avgpool(graph['conv4_4'])\n",
    "    graph['conv5_1']  = _conv2d_relu(vgg_layers, graph['avgpool4'], 28, 'conv5_1')\n",
    "    graph['conv5_2']  = _conv2d_relu(vgg_layers, graph['conv5_1'], 30, 'conv5_2')\n",
    "    graph['conv5_3']  = _conv2d_relu(vgg_layers, graph['conv5_2'], 32, 'conv5_3')\n",
    "    graph['conv5_4']  = _conv2d_relu(vgg_layers, graph['conv5_3'], 34, 'conv5_4')\n",
    "    graph['avgpool5'] = _avgpool(graph['conv5_4'])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1014, 1280, 3)\n",
      "(256, 256, 3)\n",
      "(1, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "style_image = scipy.misc.imread('starry_night.jpg')\n",
    "print(style_image.shape)\n",
    "style_image = scipy.misc.imresize(style_image, (256,256,3))\n",
    "print(style_image.shape)\n",
    "style_image = style_image - MEAN_PIXEL\n",
    "style_image = np.asarray(style_image, np.float32)\n",
    "style_image = np.expand_dims(style_image, axis = 0)\n",
    "print(style_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_image = tf.placeholder(tf.float32, shape=(None,256,256,3), name='style_image')\n",
    "vgg_net = load_vgg('imagenet-vgg-verydeep-19.mat', input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_features = {}\n",
    "with tf.Session() as sess :\n",
    "    for layer in STYLE_LAYERS :\n",
    "        tmp = sess.run(vgg_net[layer], feed_dict = {input_image : style_image})\n",
    "        tmp = np.reshape(tmp, (-1,tmp.shape[3]))\n",
    "        gram = np.matmul(tmp.T, tmp)/float(tmp.size)\n",
    "        style_features[layer] = gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_features['conv1_1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_style_net(input_image):\n",
    "    graph = {}\n",
    "    graph['conv1'] = _conv_layer(input_image, 32, 9, 1)\n",
    "    graph['conv2'] = _conv_layer(graph['conv1'], 64, 3, 2)\n",
    "    graph['conv3'] = _conv_layer(graph['conv2'], 128, 3, 2)\n",
    "    graph['resid1'] = _residual_block(graph['conv3'], 3)\n",
    "    graph['resid2'] = _residual_block(graph['resid1'], 3)\n",
    "    graph['resid3'] = _residual_block(graph['resid2'], 3)\n",
    "    graph['resid4'] = _residual_block(graph['resid3'], 3)\n",
    "    graph['resid5'] = _residual_block(graph['resid4'], 3)\n",
    "    graph['conv_t1'] = _conv_tranpose_layer(graph['resid5'], 64, 3, 2)\n",
    "    graph['conv_t2'] = _conv_tranpose_layer(graph['conv_t1'], 32, 3, 2)\n",
    "    graph['conv_t3'] = _conv_layer(graph['conv_t2'], 3, 9, 1, relu=False)\n",
    "    graph['preds'] = tf.nn.tanh(graph['conv_t3']) * 150.0 + 255./2\n",
    "    return graph\n",
    "\n",
    "def _conv_layer(net, num_filters, filter_size, strides, relu=True):\n",
    "    weights_init = _conv_init_vars(net, num_filters, filter_size)\n",
    "    strides_shape = [1, strides, strides, 1]\n",
    "    net = tf.nn.conv2d(net, weights_init, strides_shape, padding='SAME')\n",
    "    net = _instance_norm(net)\n",
    "    if relu:\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "    return net\n",
    "\n",
    "def _conv_tranpose_layer(net, num_filters, filter_size, strides):\n",
    "    weights_init = _conv_init_vars(net, num_filters, filter_size, transpose=True)\n",
    "\n",
    "    batch_size, rows, cols, in_channels = [i.value for i in net.get_shape()]\n",
    "    new_rows, new_cols = int(rows * strides), int(cols * strides)\n",
    "    # new_shape = #tf.pack([tf.shape(net)[0], new_rows, new_cols, num_filters])\n",
    "\n",
    "    new_shape = [batch_size, new_rows, new_cols, num_filters]\n",
    "    tf_shape = tf.stack(new_shape)\n",
    "    strides_shape = [1,strides,strides,1]\n",
    "    net = tf.nn.conv2d_transpose(net, weights_init, new_shape, strides_shape, padding='SAME')\n",
    "    net = _instance_norm(net)\n",
    "    return tf.nn.relu(net)\n",
    "\n",
    "def _residual_block(net, filter_size=3):\n",
    "    tmp = _conv_layer(net, 128, filter_size, 1)\n",
    "    return net + _conv_layer(tmp, 128, filter_size, 1, relu=False)\n",
    "\n",
    "def _instance_norm(net, train=True):\n",
    "    batch, rows, cols, channels = [i.value for i in net.get_shape()]\n",
    "    var_shape = [channels]\n",
    "    mu, sigma_sq = tf.nn.moments(net, [1,2], keep_dims=True)\n",
    "    shift = tf.Variable(tf.zeros(var_shape))\n",
    "    scale = tf.Variable(tf.ones(var_shape))\n",
    "    epsilon = 1e-3\n",
    "    normalized = (net-mu)/(sigma_sq + epsilon)**(.5)\n",
    "    return scale * normalized + shift\n",
    "\n",
    "def _conv_init_vars(net, out_channels, filter_size, transpose=False):\n",
    "    _, rows, cols, in_channels = [i.value for i in net.get_shape()]\n",
    "    if not transpose:\n",
    "        weights_shape = [filter_size, filter_size, in_channels, out_channels]\n",
    "    else:\n",
    "        weights_shape = [filter_size, filter_size, out_channels, in_channels]\n",
    "\n",
    "    weights_init = tf.Variable(tf.truncated_normal(weights_shape, stddev=WEIGHTS_INIT_STDEV, seed=1), dtype=tf.float32)\n",
    "    return weights_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_content = tf.placeholder(tf.float32, shape=(BATCH_SIZE,256,256,3), name='input_batch')\n",
    "X_content = X_content - MEAN_PIXEL\n",
    "content_features = load_vgg('imagenet-vgg-verydeep-19.mat', X_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "styled_image = load_style_net(X_content/255.)\n",
    "X_preds = styled_image['preds'] - MEAN_PIXEL\n",
    "net = load_vgg('imagenet-vgg-verydeep-19.mat', X_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = net[CONTENT_LAYER].get_shape().as_list()\n",
    "content_size = tmp[0]*tmp[1]*tmp[2]*tmp[3]\n",
    "content_loss = CONTENT_WEIGHT * (2 * tf.nn.l2_loss(content_features[CONTENT_LAYER]-net[CONTENT_LAYER]))/content_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_loss = 0.0\n",
    "for style_layer in STYLE_LAYERS:\n",
    "    layer = net[style_layer]\n",
    "    bs, height, width, filters = layer.get_shape().as_list()\n",
    "    size = height * width * filters\n",
    "    feats = tf.reshape(layer, (bs, height * width, filters))\n",
    "    feats_T = tf.transpose(feats, perm=[0,2,1])\n",
    "    grams = tf.matmul(feats_T, feats) / size\n",
    "    style_gram = style_features[style_layer]\n",
    "    style_losses = tf.add(style_loss, (2 * tf.nn.l2_loss(grams - style_gram)/style_gram.size))\n",
    "style_loss = STYLE_WEIGHT * style_loss/ BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# total variation denoising\n",
    "batch_shape = [BATCH_SIZE, 256,256,3]\n",
    "tmp = X_preds[:,1:,:,:].get_shape().as_list()[1:]\n",
    "tv_y_size = tmp[0]*tmp[1]*tmp[2]\n",
    "tmp = X_preds[:,:,1:,:].get_shape().as_list()[1:]\n",
    "tv_x_size = tmp[0]*tmp[1]*tmp[2]\n",
    "y_tv = tf.nn.l2_loss(X_preds[:,1:,:,:] - X_preds[:,:batch_shape[1]-1,:,:])\n",
    "x_tv = tf.nn.l2_loss(X_preds[:,:,1:,:] - X_preds[:,:,:batch_shape[2]-1,:])\n",
    "tv_loss = TV_WEIGHT * 2 * (x_tv/tv_x_size + y_tv/tv_y_size)/BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_loss = content_loss + style_loss + tv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer =  tf.train.AdamOptimizer(LEARNING_RATE).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################   DATASET  #############################\n",
    "def next_batch() :\n",
    "    files = os.listdir('./train2014')\n",
    "    #print('Total images : ', len(files))\n",
    "    files_rem = len(files)%BATCH_SIZE\n",
    "    files = files[:-files_rem]\n",
    "    print('Total images : ', len(files))\n",
    "    for i in xrange(0,len(files),BATCH_SIZE) :\n",
    "        tmp = np.zeros((BATCH_SIZE, 256, 256, 3))\n",
    "        for j in xrange(BATCH_SIZE) :\n",
    "            image = files[i+j]\n",
    "            tmp2 = scipy.misc.imread('./train2014/'+image)\n",
    "            tmp2 = scipy.misc.imresize(tmp2,(256,256,3))\n",
    "            tmp2 = np.asarray(tmp2,dtype = np.float32)\n",
    "            if len(tmp2.shape) != 3 :\n",
    "                tmp[j,:,:,0] = tmp2[:,:]\n",
    "                tmp[j,:,:,1] = tmp2[:,:]\n",
    "                tmp[j,:,:,2] = tmp2[:,:]\n",
    "            else :\n",
    "                tmp[j,:,:,:] = tmp2[:,:,:]\n",
    "        \n",
    "        yield np.asarray(tmp, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('EPOCH : ', 0)\n",
      "('Total images : ', 82780)\n",
      "('total_loss in batch No.', 0, 2055245.25)\n",
      "('EPOCH : ', 1)\n",
      "('Total images : ', 82780)\n",
      "('total_loss in batch No.', 10, 354203.375)\n",
      "('total_loss in batch No.', 20, 218170.03125)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-29482845dadc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'input_batch:0'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total_loss in batch No.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    i = 0\n",
    "    for epoch in xrange(NUM_EPOCHS) :\n",
    "        print(\"EPOCH : \",epoch)\n",
    "        tmp = 0\n",
    "        for X_batch in next_batch() :\n",
    "            _,l = sess.run([optimizer,total_loss], feed_dict = {'input_batch:0' : X_batch})\n",
    "            if i%10 == 0:\n",
    "                print('total_loss in batch No.',i,l/BATCH_SIZE)\n",
    "            i += 1\n",
    "            if i%5000 == 1 :\n",
    "                tmp = sess.run(styled_image, feed_dict = {'input_batch:0' : X_batch})\n",
    "                tmp = tmp['preds']\n",
    "                scipy.misc.imsave('./samples/_'+str(epoch)+'_'+str(i)+'_1.png',tmp[0])\n",
    "                scipy.misc.imsave('./samples/_'+str(epoch)+'_'+str(i)+'_2.png',tmp[1])\n",
    "                scipy.misc.imsave('./samples/_'+str(epoch)+'_'+str(i)+'_3.png',tmp[2])\n",
    "                scipy.misc.imsave('./samples/_'+str(epoch)+'_'+str(i)+'_4.png',tmp[3])\n",
    "                saver.save(sess, './models/model',global_step=epoch)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
